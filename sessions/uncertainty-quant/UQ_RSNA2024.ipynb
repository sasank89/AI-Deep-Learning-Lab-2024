{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RSNA/AI-Deep-Learning-Lab-2024/blob/main/sessions/uncertainty-quant/UQ_RSNA2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RSNA Deep Learning Lab: Quantifying Uncertainty in Deep Learning\n",
        "##Shahriar Faghani MD, Mana Moassefi MD, Bradley J. Erickson, MD., PhD.\n",
        "###Radiology Informatics Lab, Department of Radiology, Mayo Clinic, Rochester, MN"
      ],
      "metadata": {
        "id": "0jaTFuOa31rg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KPPEciuXYeF"
      },
      "source": [
        "## Packages and Imports\n",
        "In this tutorial, you will learn how to train uncertainty aware models using Enemble, Monte Carlo Dropout, Evidential, and Conformal techniques. To start with this notebook, you need to install three packages:\n",
        "- MONAI: A framework for deep learning in medical image analysis.\n",
        "- timm: A rich collection of many deep learning models pretrained on natural images.\n",
        "- NetCal: A package for model output calibration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRRw2EyZXYeH"
      },
      "outputs": [],
      "source": [
        "!pip install netcal timm monai -q --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKotNSrpXYeI"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import monai as mn\n",
        "import numpy as np\n",
        "import torch\n",
        "import timm\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.calibration import calibration_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUiFqaclXYeJ"
      },
      "source": [
        "## Data Preperation\n",
        "For this notebook we will use a dataset from [Kermany et. al.](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia?datasetId=17810) that has a set of more than 5,000 pediatric chest radiographs, with and without pneumonia. We use MONAI for dataset preperation, as it has several medical image-specific augmentations and enables using them with a very simple interface.\n",
        "\n",
        "We create our datasets and dataloaders once here and use them in all of the upcomming experiments. In brief, the following cell takes care of the following:\n",
        "\n",
        "1. Gets the list of files in the train, validation, and test folders of the dataset.\n",
        "2. Creates a dictionary of these file paths with their corresponding label (normal or pneumonia) to leverage MONAI's dictionary-based augmentation pipeline.\n",
        "3. Defines training and validation transforms.\n",
        "4. Creates datasets and dataloaders for each of the three sets (train, validation, and test)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the zip archive containing training images\n",
        "if not os.path.isdir('chest_xray'):\n",
        "    gdown.download(\n",
        "        \"https://drive.google.com/uc?export=download&confirm=pbef&id=1L8ox5fIwb_PijLcPEofQyhe3oGiYESO2\",\n",
        "        \"chest_xray.zip\",\n",
        "        quiet=True\n",
        "    )\n",
        "    !unzip -q chest_xray.zip\n",
        "    os.remove('chest_xray.zip')"
      ],
      "metadata": {
        "id": "Li99VwP4WjZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxVvS-mvXYeJ"
      },
      "outputs": [],
      "source": [
        "# Global Variables\n",
        "DATA_DIR = 'chest_xray'\n",
        "MONAI_CACHE_DIR = 'chest_xray/cache_UQ'\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "DROP_RATE = 0.3\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_ARCHITECTURE = \"convnext_small\"\n",
        "\n",
        "# 1. Gets the list of files in the train, validation, and test folders of the dataset.\n",
        "train_normal = glob(f\"{DATA_DIR}/train/NORMAL/*.jpeg\")\n",
        "train_pneumonia = glob(f\"{DATA_DIR}/train/PNEUMONIA/*.jpeg\")\n",
        "os.mkdir(f\"{DATA_DIR}/val\")\n",
        "os.mkdir(f\"{DATA_DIR}/val/NORMAL\")\n",
        "os.mkdir(f\"{DATA_DIR}/val/PNEUMONIA\")\n",
        "for file in train_normal[:10]:\n",
        "  shutil.move(file,(f\"{DATA_DIR}/val/NORMAL\"))\n",
        "for file in train_pneumonia[:10]:\n",
        "  shutil.move(file,(f\"{DATA_DIR}/val/PNEUMONIA\"))\n",
        "train_files = glob(f\"{DATA_DIR}/train/*/*.jpeg\")\n",
        "val_files = glob(f\"{DATA_DIR}/val/*/*.jpeg\")\n",
        "test_files = glob(f\"{DATA_DIR}/test/*/*.jpeg\")\n",
        "\n",
        "# 2. Creates a dictionary of image paths and labels.\n",
        "def get_data_dict(files:list)-> dict:\n",
        "    \"\"\"creates a dictionary of image paths and labels\n",
        "\n",
        "    Args:\n",
        "        files (list): list of image paths\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of image paths and labels\n",
        "    \"\"\"\n",
        "    final_dict = []\n",
        "    for file in files:\n",
        "        label = file.split(\"/\")[-2]\n",
        "        if label == \"NORMAL\":\n",
        "            label = 0\n",
        "        else:\n",
        "            label = 1\n",
        "        final_dict.append({\"img\": file, \"label\": label})\n",
        "\n",
        "    return final_dict\n",
        "\n",
        "train_data = get_data_dict(train_files)\n",
        "val_data = get_data_dict(val_files)\n",
        "test_data = get_data_dict(test_files)\n",
        "\n",
        "# 3. Defines training and validation transforms.\n",
        "train_transforms = mn.transforms.Compose([\n",
        "    mn.transforms.LoadImaged(keys=[\"img\"]),\n",
        "    mn.transforms.EnsureChannelFirstD(keys=[\"img\"]),\n",
        "    mn.transforms.LambdaD(keys=[\"img\"], func=lambda x: x[0:1, :,:]),\n",
        "    mn.transforms.ScaleIntensityd(keys=[\"img\"],minv=0.0, maxv=1.0,),\n",
        "    mn.transforms.ResizeD(keys=[\"img\"], spatial_size=IMG_SIZE, size_mode=\"longest\"),\n",
        "    mn.transforms.SpatialPadD(keys=[\"img\"], spatial_size=IMG_SIZE, method=\"symmetric\"),\n",
        "    mn.transforms.RandAffineD(keys='img', rotate_range=0.25, translate_range=int(IMG_SIZE * 0.05), scale_range=0.1, mode=\"bilinear\", padding_mode=\"zeros\", prob=0.8),\n",
        "    mn.transforms.RandFlipD(keys='img', spatial_axis=1, prob=0.5),\n",
        "    mn.transforms.RandGaussianNoiseD(keys='img', mean=0.5, std=0.2, prob=0.5),\n",
        "    mn.transforms.ToTensorD(keys=[\"img\"]),\n",
        "    mn.transforms.ToTensorD(keys=[\"label\"], dtype=torch.long),\n",
        "    mn.transforms.SelectItemsD(keys=[\"img\",\"label\"]),\n",
        "])\n",
        "\n",
        "val_transforms = mn.transforms.Compose([\n",
        "    mn.transforms.LoadImaged(keys=[\"img\"]),\n",
        "    mn.transforms.EnsureChannelFirstD(keys=[\"img\"]),\n",
        "    mn.transforms.LambdaD(keys=[\"img\"], func=lambda x: x[0:1, :,:]),\n",
        "    mn.transforms.ResizeD(keys=[\"img\"], spatial_size=IMG_SIZE, size_mode=\"longest\"),\n",
        "    mn.transforms.SpatialPadD(keys=[\"img\"], spatial_size=IMG_SIZE, method=\"symmetric\"),\n",
        "    mn.transforms.ToTensorD(keys=[\"img\"]),\n",
        "    mn.transforms.ToTensorD(keys=[\"label\"], dtype=torch.long),\n",
        "    mn.transforms.SelectItemsD(keys=[\"img\",\"label\"]),\n",
        "])\n",
        "\n",
        "# 4. Creates a dataset and a dataloader.\n",
        "train_ds = mn.data.PersistentDataset(data=train_data[:2000], transform=train_transforms, cache_dir=MONAI_CACHE_DIR)\n",
        "val_ds = mn.data.PersistentDataset(data=test_data, transform=val_transforms, cache_dir=MONAI_CACHE_DIR)\n",
        "test_ds = mn.data.Dataset(data=val_data, transform=val_transforms)\n",
        "\n",
        "def get_dataloaders(train_ds=train_ds, val_ds=val_ds, test_ds=test_ds, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Creates a dataloader for the train, validation, and test datasets.\n",
        "\n",
        "    Args:\n",
        "        train_ds (PersistentDataset, optional): Training dataset. Defaults to train_ds.\n",
        "        val_ds (PersistentDataset, optional): Validation dataset. Defaults to val_ds.\n",
        "        test_ds (Dataset, optional): Test dataset. Defaults to test_ds.\n",
        "        batch_size (int, optional): Batch size. Defaults to BATCH_SIZE.\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: Dataloader for the train dataset.\n",
        "        DataLoader: Dataloader for the validation dataset.\n",
        "        DataLoader: Dataloader for the test dataset.\n",
        "    \"\"\"\n",
        "    def seed_worker(worker_id):\n",
        "        worker_seed = torch.initial_seed() % 2**32\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, worker_init_fn=seed_worker)\n",
        "    val_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, worker_init_fn=seed_worker)\n",
        "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False, drop_last=False, num_workers=2, worker_init_fn=seed_worker)\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6w47XiVXYeL"
      },
      "source": [
        "## Model Training\n",
        "In this tutorial we will be dealing with several models and need to have a robust pipeline for training our deep learning models. In order to facilitate this process, we will create a function that takes care of training our models given the architecture, training seed and loss function as parameters. First we have to have some helper functions that take care of seeding training parameters (including the augmentation pipeline), calculating area under receiver operating curve (ROC), and training on a single batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k60YuvdkXYeL"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "    \"\"\"sets the seed for all libraries\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): seed value. Defaults to 42.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    mn.utils.misc.set_determinism(seed=seed)\n",
        "\n",
        "\n",
        "class AUCMetric(object):\n",
        "    \"\"\"\n",
        "    Computes the Area Under the ROC (AUROC) for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.y_preds = []\n",
        "        self.y_trues = []\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        self.y_preds.append(y_pred)\n",
        "        self.y_trues.append(y_true)\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_preds = []\n",
        "        self.y_trues = []\n",
        "\n",
        "    def compute(self):\n",
        "        y_preds = torch.cat(self.y_preds, dim=0).cpu().numpy()[:, 1]\n",
        "        y_trues = torch.cat(self.y_trues, dim=0).cpu().numpy()\n",
        "        auc = roc_auc_score(y_trues, y_preds)\n",
        "        return auc\n",
        "\n",
        "def one_step(model, batch, loss_fn, optimizer, device: str = \"cuda:0\", training: bool = True):\n",
        "    \"\"\"trains the model for one step\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): model\n",
        "        batch (dict): batch of data\n",
        "        loss_fn (torch.nn.Module): loss function\n",
        "        optimizer (torch.optim.Optimizer): optimizer\n",
        "        training (bool, optional): whether the model is in training mode or not. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        float: loss value\n",
        "    \"\"\"\n",
        "    x = batch[\"img\"].to(device)\n",
        "    y = batch[\"label\"].to(device)\n",
        "\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    with torch.set_grad_enabled(training):\n",
        "        y_pred = model(x)\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        if training:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    return loss.item(), torch.softmax(y_pred, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A-0InDIXYeM"
      },
      "source": [
        "Now we can write our generic training function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d_d-9dhXYeM"
      },
      "outputs": [],
      "source": [
        "def train_model(architecture, drop_rate, loss_fn, weight_filename: str = \"best_model\", num_classes: int = 2, training_seed: int = 42, epochs: int = 10, device: str = \"cuda:0\"):\n",
        "    \"\"\"trains the model\n",
        "\n",
        "    Args:\n",
        "        architecture (torch.nn.Module): model architecture\n",
        "        train_dataloader (torch.utils.data.DataLoader): train dataloader\n",
        "        val_dataloader (torch.utils.data.DataLoader): validation dataloader\n",
        "        loss_fn (torch.nn.Module): loss function\n",
        "        training_seed (int): seed value\n",
        "        epochs (int, optional): number of epochs. Defaults to 10.\n",
        "        device (str, optional): device to use. Defaults to \"cuda:0\".\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: best weights of the trained model\n",
        "    \"\"\"\n",
        "    # some hyperparameters that you can use to tune your model\n",
        "    lr = 1e-4\n",
        "    weight_decay = 1e-5\n",
        "\n",
        "    seed_everything(training_seed)\n",
        "\n",
        "    model = timm.create_model(architecture, drop_rate=drop_rate, pretrained=False, in_chans=1, num_classes=num_classes).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_auc = 0\n",
        "    best_model = None\n",
        "    validation_auc = AUCMetric()\n",
        "\n",
        "    train_dataloader, val_dataloader, _ = get_dataloaders()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        for batch in tqdm(train_dataloader, desc=\"training\"):\n",
        "            train_loss, _ = one_step(model, batch, loss_fn, optimizer, device=device, training=True)\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "        validation_auc.reset()\n",
        "        for batch in tqdm(val_dataloader, desc=\"validating\"):\n",
        "            val_loss, y_pred = one_step(model, batch, loss_fn, optimizer, device=device, training=False)\n",
        "            val_losses.append(val_loss)\n",
        "            validation_auc(y_pred, batch[\"label\"])\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        val_loss = np.mean(val_losses)\n",
        "        val_auc = validation_auc.compute()\n",
        "        print(f\"train loss: {train_loss:.4f}\")\n",
        "        print(f\"val loss: {val_loss:.4f}\")\n",
        "        print(f\"val auc: {val_auc:.4f}\")\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_model = copy.deepcopy(model)\n",
        "            print(\"saving best model\")\n",
        "            torch.save(best_model.state_dict(), f\"{weight_filename}.pth\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcrgr3EDXYeN"
      },
      "source": [
        "Let's train a simple model with `CrossEntropy` loss, which is the routine practice in many deep learning projects. `CrossEntropy` loss is particularly well-suited for binary classification tasks because it has a steeper gradient when the predicted label (ŷ) is far from the ground truth (y) compared to other popular loss function such as Mean Squared Error (MSE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lU19yDKXYeN"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "best_model = train_model(\n",
        "    architecture=MODEL_ARCHITECTURE,\n",
        "    drop_rate=DROP_RATE,\n",
        "    loss_fn=loss_fn,\n",
        "    weight_filename=\"CE_seed_42\",\n",
        "    training_seed=42,\n",
        "    num_classes=2,\n",
        "    epochs=EPOCHS,\n",
        "    device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZzXhDNXYeO"
      },
      "source": [
        "Now let's checkout our model's calibration curve:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-8QYXUsXYeO"
      },
      "outputs": [],
      "source": [
        "def get_inference_results(weight_path, set_name='val'):\n",
        "    model = timm.create_model(MODEL_ARCHITECTURE, pretrained=False, in_chans=1, num_classes=2).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(weight_path))\n",
        "    model.eval()\n",
        "    if set_name == 'val':\n",
        "        _, dataloader, _ = get_dataloaders()\n",
        "    elif set_name == 'test':\n",
        "        _, _, dataloader = get_dataloaders()\n",
        "    else:\n",
        "        raise ValueError('set_name must be val or test')\n",
        "    y_preds = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            x = batch[\"img\"].to(DEVICE)\n",
        "            y = batch[\"label\"].to(DEVICE)\n",
        "            y_pred = model(x)\n",
        "            y_preds.extend(torch.softmax(y_pred, dim=-1).cpu().numpy())\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "\n",
        "    y_preds = np.array(y_preds)\n",
        "    y_true = np.array(y_true)\n",
        "    return np.array(y_preds), np.array(y_true)\n",
        "\n",
        "y_preds, y_true = get_inference_results(\"CE_seed_42.pth\", set_name='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYemFe1vXYeP"
      },
      "outputs": [],
      "source": [
        "def plot_calibration_curve(y_true, y_pred, n_bins=10, normalize=True, title=\"Calibration Curve\", ax=None):\n",
        "    \"\"\"plots calibration curve\n",
        "\n",
        "    Args:\n",
        "        y_true (np.array): true labels\n",
        "        y_pred (np.array): predicted labels\n",
        "        n_bins (int, optional): number of bins. Defaults to 10.\n",
        "        normalize (bool, optional): whether to normalize the data or not. Defaults to True.\n",
        "        title (str, optional): title of the plot. Defaults to \"Calibration Curve\".\n",
        "        ax (matplotlib.axes.Axes, optional): axes to plot on. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.axes.Axes: axes\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred, n_bins=n_bins)\n",
        "    ax.plot(mean_predicted_value, fraction_of_positives, \"s-\")\n",
        "    ax.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
        "    ax.set_ylabel(\"Fraction of positives\")\n",
        "    ax.set_ylim([-0.05, 1.05])\n",
        "    ax.set_xlabel(\"Mean predicted value\")\n",
        "    ax.set_title(title)\n",
        "    return ax\n",
        "\n",
        "plot_calibration_curve(y_true, y_preds[:, 1], title=\"Calibration Curve (before calibration)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46-kulrHXYeP"
      },
      "source": [
        "## Calibration\n",
        "\n",
        "One of the cornerstones of uncertainty quantification is probability calibration. Traditionally, deep learning models output some logits ŷ for which each ŷ<sub>i</sub> ∈ ℝ. Then, we \"activate\" using a function like [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) or [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) these ŷ such that each ŷ<sub>i</sub> is now bounded on the interval [0, 1]. Furthermore, if there are K classes, the sum over all K activated ŷ<sub>i</sub> equals 1. With these properties, we often interpret the activated outputs of a model as class probabilities. Concretely, in our task, if the ConvNet outputs a tensor of activated predictions ŷ = [0.3, 0.7], we would assign our sample i a label of `1`, since the argmax of ŷ is `1`, and we would say that our model assigns a 70% probability to the event that our sample's true class is `1`.\n",
        "\n",
        "This, however, is a critical misconception. In reality, the activated outputs of our model, while they posses the mathematical properties of probabilties, cannot be interpreted as such until they are calibrated. In general, probability calibration refers to returning the *true* likelihood of an event. Hence, since our events can be defined as a sample having a label `0` and having a label `1`, probability calibration is the process of aligning our model's activated outputs with the *true* likelihood of the sample having each label.\n",
        "\n",
        "To calibrate our model's predictions, we use the `netcal` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4abtIeg6XYeP"
      },
      "outputs": [],
      "source": [
        "from netcal.binning import IsotonicRegression\n",
        "\n",
        "calibrator = IsotonicRegression()\n",
        "calibrator.fit(y_preds, y_true)\n",
        "y_preds_calibrated = calibrator.transform(y_preds)\n",
        "plot_calibration_curve(y_true, y_preds_calibrated);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIcVwseLXYeP"
      },
      "source": [
        "## UQ Techniques: Ensemble\n",
        "The ensemble technique relies on the instrinsic variability of deep learning models. An ensemble is a collection of models of the same architecture (usually, but not always) whose predictions are aggregated in a *soft* or *hard* manner to output a single point prediction (for classification tasks, that is). Soft prediction refers to the aggregation of the models' activated outputs, while hard aggregation accepts the models' class predictions (discrete integers 0,...,K-1). In this tutorial, we employ soft prediction.\n",
        "\n",
        "Most critically, however, **the variance of the ensemble's predictions is calculated and interpreted as a measure of uncertainty**.\n",
        "\n",
        "One analogy to help build intuition behind the ensemble UQ technique is a panel of doctors. Assume each doctor received his or her medical education from the same institution, completed their residency in the same hospital system, and now specializes in the same field. When presented with a sample, if the panel has unanimous or near-unanimous agreement about the sample's label, we would consider the panel to be highly certain. And conversely, if there are many dissenters or high variability among the opinions of each panelist, we would consider their ruling to be highly uncertain. Now, replace the panelists with deep learning models and medical school with a training dataset, and you understand the ensemble UQ technique!\n",
        "\n",
        "The strength of the ensemble is its conceptual simplicity. The straightforwardness of variance as a measure of uncertainty can help people grasp the intuition of the ensemble without needing an advanced background in mathematics.\n",
        "\n",
        "This simplicity, however, comes with a cost. The ensemble's primary weakness is that it is not mathematically rigorous. Of course, a UQ technique need not be mathematically complex nor convoluted to be effective, but the ensemble assigns a quantity an entirely unsupported interpretation when it considers variance a measure of uncertainty. Just because there are qualitative similarities between real-life \"ensembles\" and deep learning ensembles does not mean that we can place meaning where it does not exist. Hence, experts generally consider ensembles to be a naive approach to UQ. Ensembles are also significantly more computationally intensive than single-model UQ techniques. Nevertheless, let's try one out!\n",
        "\n",
        "To use this technique, we have to train our model with different initialization seeds. For increased variance, we can also train each model on a different fold from our training set, though we do not demonstrate that in this tutorial. This process is known as *cross validation*.\n",
        "\n",
        "Then we can run a single image through all of those models and get a mean and standard deviation of their prediction. *The higher the standard deviation, the more uncertain the models are.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ31emQTXYeP"
      },
      "outputs": [],
      "source": [
        "ensemble_models = [best_model]\n",
        "training_seeds = [42, 43]\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "for seed in training_seeds[1:]: # we already trained the first model (seed=42)\n",
        "    print(\"*\" * 40)\n",
        "    print(f\"Training model with seed {seed}\")\n",
        "    print(\"*\" * 40)\n",
        "    ensemble_models.append(train_model(\n",
        "        architecture=MODEL_ARCHITECTURE,\n",
        "        loss_fn=loss_fn,\n",
        "        drop_rate=DROP_RATE,\n",
        "        weight_filename=f\"seed_{seed}\",\n",
        "        training_seed=seed,\n",
        "        num_classes=2,\n",
        "        epochs=EPOCHS,\n",
        "        device=DEVICE\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyL0n5bSXYeQ"
      },
      "outputs": [],
      "source": [
        "_, _, test_dataloader = get_dataloaders()\n",
        "for i, item in enumerate(test_dataloader):\n",
        "    if i >= 3 and i < (len(test_dataloader) - 3):\n",
        "        continue # we only need a few samples\n",
        "\n",
        "    img = item[\"img\"].to(DEVICE)\n",
        "    label = item[\"label\"].to(DEVICE)\n",
        "    y_preds = []\n",
        "    for model in tqdm(ensemble_models):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(img)\n",
        "            y_preds.append(torch.softmax(y_pred, dim=-1).cpu().numpy()[:, 1].item())\n",
        "    y_preds = np.array(y_preds)\n",
        "    mean_pred = np.mean(y_preds)\n",
        "    std_pred = np.std(y_preds)\n",
        "    print(f\"Predicted label: {mean_pred:.4f} (SD: {std_pred:.4f})\")\n",
        "    print(f\"True label: {label.item()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyap7vY0XYeQ"
      },
      "source": [
        "## UQ Techniques: Monte Carlo Dropout\n",
        "Monte Carlo Dropout for UQ is mathematically similar to the ensemble technique because it relies on the variance of predictions to quantify uncertainty. The training process, however, requires only one model. It is at inference (test) time when variability is introduced Monte Carlo Dropout. Dropout refers to \"turning off\" randomly selected nodes in a model. By instantiating models with different nodes turned off, we can observe the variability of predictions within a single model's architecture. We perform dropout on a model several times, which gives us many instance of that model. Then, we use those unique instances to infer on a sample and calculate the variance of the instances' predictions to be the model's uncertainty.\n",
        "\n",
        "Monte Carlo Dropout generally has comparatively poor performance, and it, like the ensemble technique, is not mathematically rigorous because it defines uncertainty as variance.\n",
        "\n",
        "To use this technique, we turn on our model's drop layer(s) during inference and run the image through the model multiple times, this leads to having multiple predictions for a signle image. Similar to the ensemble technique we can have the mean and standard deviation of these predictions an use them as a measure of uncertainty. *The higher the standard deviation, the more uncertain a model is.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_nrvABRXYeQ"
      },
      "outputs": [],
      "source": [
        "model = best_model\n",
        "model.eval()\n",
        "number_of_iterations = 10 # number of iterations to run the dropout\n",
        "# turn on dropout\n",
        "for m in model.modules():\n",
        "    if m.__class__.__name__.startswith(\"Dropout\"):\n",
        "        m.train()\n",
        "\n",
        "_, _, test_dataloader = get_dataloaders()\n",
        "for i, item in enumerate(test_dataloader):\n",
        "    if i >= 3 and i < (len(test_dataloader) - 3):\n",
        "        continue # we only need a few samples\n",
        "\n",
        "    img = item[\"img\"].to(DEVICE)\n",
        "    label = item[\"label\"].to(DEVICE)\n",
        "    y_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(number_of_iterations):\n",
        "            y_pred = model(img)\n",
        "            y_preds.append(torch.softmax(y_pred, dim=-1).cpu().numpy()[:, 1].item())\n",
        "\n",
        "    y_preds = np.array(y_preds)\n",
        "    mean_pred = np.mean(y_preds)\n",
        "    std_pred = np.std(y_preds)\n",
        "    print(f\"Predicted label: {mean_pred:.4f} (SD: {std_pred:.4f})\")\n",
        "    print(f\"True label: {label.item()}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjpol_PXYeQ"
      },
      "source": [
        "## UQ Techniques: Evidential Deep Learning\n",
        "\n",
        "Evidential Deep Learning (EDL) is a UQ technique that implements a custom loss function to calculate the uncertainty as inversely proportional (additively) to the amount of evidence obtained by the model. EDL transforms the training process into an evidence acquisition process by attempting to maximize the amount of evidence the model gathers for correct predictions, thus reducing its uncertainty when it accurately classifies a sample. EDL works by passing the logits generated by our model as the alpha parameters of a Dirichlet distribution. A Dirichlet distribution is an advanced statistics concept that is a second-order probability map. In essence, it gives information of about the \"probability of probabilities.\" For example, we are all familiar with first-order probability questions such as a fair coin being flipped some number of times. But what about an unfair coin whose probability of heads varies with every flip? A Dirichlet distribution would describe the probability of a given chance of heads (e.g. P(P(heads) = 0.5)). Applying this to deep learning problems allows us to calculate a model's uncertainty (even during training!) in a way that is both mathematically rigorous and not computationally costly.\n",
        "\n",
        "Using evidential deep learning (EDL) requires that our model be retrained with a new loss function.  The following cell demonstrates how this loss function is defined:\n",
        "\n",
        "---\n",
        ">\n",
        "There are multiple ways to define the EDL loss function. For an exhaustive discussion of this topic, please refer to the [original paper](https://arxiv.org/pdf/1806.01768.pdf).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggUlGPDiXYeQ"
      },
      "outputs": [],
      "source": [
        "class EDLLossv1(torch.nn.Module):\n",
        "    def __init__(self, num_classes=2, regr=1e-5, to_one_hot=True):\n",
        "        self.regr = regr # regularization parameter\n",
        "        self.one_hot = to_one_hot\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __call__(self, y_preds, y_true):\n",
        "        if self.one_hot:\n",
        "            y_true = torch.nn.functional.one_hot(y_true, num_classes=self.num_classes).float()\n",
        "        evidence = torch.nn.functional.softplus(y_preds, beta=0.1)\n",
        "        alpha = evidence + 1\n",
        "        alpha_sum = torch.sum(alpha, dim=1, keepdim=True)\n",
        "        entropy_reg = torch.distributions.dirichlet.Dirichlet(alpha).entropy()\n",
        "        edl_loss = torch.sum(y_true * (torch.digamma(alpha_sum) - torch.digamma(alpha))) - self.regr * torch.sum(entropy_reg)\n",
        "        return edl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjJQ_QwsXYeQ"
      },
      "outputs": [],
      "source": [
        "class EDLLossv2(torch.nn.Module):\n",
        "    def __init__(self, annealing_step=0, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.annealing_step = annealing_step\n",
        "\n",
        "    def _get_evidence(self, y):\n",
        "        return torch.nn.functional.softplus(y)\n",
        "\n",
        "    def _kl_divergence(self, alpha):\n",
        "        device = alpha.device\n",
        "        ones = torch.ones([1, self.num_classes], dtype=torch.float32, device=device)\n",
        "        alpha_sum = torch.sum(alpha, dim=1, keepdim=True)\n",
        "        first_term = (\n",
        "            torch.lgamma(alpha_sum)\n",
        "            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n",
        "            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n",
        "            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n",
        "        )\n",
        "        second_term = (\n",
        "            (alpha - ones)\n",
        "            .mul(torch.digamma(alpha) - torch.digamma(alpha_sum))\n",
        "            .sum(dim=1, keepdim=True)\n",
        "        )\n",
        "        kl = first_term + second_term\n",
        "        return kl\n",
        "\n",
        "    def _loglikelihood_loss(self, y, alpha):\n",
        "        device = alpha.device\n",
        "        y = y.to(device)\n",
        "        alpha = alpha.to(device)\n",
        "        alpha_sum = torch.sum(alpha, dim=1, keepdim=True)\n",
        "        loglikelihood_err = torch.sum((y - (alpha / alpha_sum)) ** 2, dim=1, keepdim=True)\n",
        "        loglikelihood_var = torch.sum(alpha * (alpha_sum - alpha) / (alpha_sum * alpha_sum * (alpha_sum + 1)), dim=1, keepdim=True)\n",
        "        loglikelihood = loglikelihood_err + loglikelihood_var\n",
        "        return loglikelihood\n",
        "\n",
        "    def __call__(self, output, target, epoch_num=None):\n",
        "        evidence = self._get_evidence(output)\n",
        "        alpha = evidence + 1\n",
        "        device = alpha.device\n",
        "        if target.ndim == 1:\n",
        "            y = torch.nn.functional.one_hot(target, num_classes=self.num_classes).float().to(device)\n",
        "        else:\n",
        "            y = target.to(device)\n",
        "        alpha = alpha.to(device)\n",
        "        loglikelihood = self._loglikelihood_loss(y, alpha)\n",
        "        if self.annealing_step!= 0:\n",
        "            assert epoch_num is not None, \"epoch_num must be provided when using annealing\"\n",
        "            annealing_coef = torch.min(\n",
        "                torch.tensor(1.0, dtype=torch.float32),\n",
        "                torch.tensor(epoch_num / self.annealing_step, dtype=torch.float32),\n",
        "            )\n",
        "        else:\n",
        "            annealing_coef = 1.0\n",
        "        kl_alpha = (alpha - 1) * (1 - y) + 1\n",
        "        kl_div = annealing_coef * self._kl_divergence(kl_alpha)\n",
        "        loss = (loglikelihood + kl_div).mean()\n",
        "        return loss\n",
        "\n",
        "# Dummy test\n",
        "loss = EDLLossv2()\n",
        "trues = torch.randint(0, 2, (10,))\n",
        "preds = torch.randn(10, 2)\n",
        "print(trues.shape, preds.shape)\n",
        "loss(preds, trues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JdBMElFXYeR"
      },
      "source": [
        "Now let's retrain our model with the EDL loss function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opFSnYYvXYeR"
      },
      "outputs": [],
      "source": [
        "loss_fn = EDLLossv1()\n",
        "edl_model = train_model(\n",
        "    architecture=MODEL_ARCHITECTURE,\n",
        "    loss_fn=loss_fn,\n",
        "    drop_rate=DROP_RATE,\n",
        "    weight_filename=f\"edl_seed_42\",\n",
        "    training_seed=42,\n",
        "    num_classes=2,\n",
        "    epochs=3,\n",
        "    device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BJu6DgMXYeR"
      },
      "source": [
        "Finally, we can apply our model to a single image and perform a handful of calculations to obtain the calibrated probabilities and the prediction's uncertainty:\n",
        "\n",
        "---\n",
        ">\n",
        "This \"handful of calculations\" is described extensively in the original paper linked above. We do not go into elaborate detail here about the specific loss function because first, there are many EDL loss functions, and second, because the focus of this tutorial is a breadth-first exploration of uncertainty quantification, rather than a thorough explanation of the mathematics of EDL.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1pv4QC0XYeR"
      },
      "outputs": [],
      "source": [
        "def get_edl_probs(y_preds):\n",
        "    if type(y_preds) != torch.Tensor:\n",
        "        y_preds = torch.tensor(y_preds)\n",
        "    evidence = torch.nn.functional.softplus(y_preds, beta = 0.1)\n",
        "    alpha = evidence + 1\n",
        "    alpha_sum = torch.sum(alpha, dim=1, keepdim=True)\n",
        "    probs = alpha / alpha_sum\n",
        "    uncertainties = y_preds.shape[1] / alpha_sum\n",
        "    return probs, uncertainties, alpha_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJPImd-aXYeR"
      },
      "outputs": [],
      "source": [
        "y_preds, y_true = get_inference_results(\"edl_seed_42.pth\")\n",
        "y_probs, uncertainties, alpha_sums = get_edl_probs(y_preds)\n",
        "\n",
        "for label, prob, uncertainty in zip(y_true, y_probs, uncertainties):\n",
        "    # print(label, prob, uncertainty)\n",
        "    print(f\"Label: {label} | Prediction: {prob.argmax()} | Probablity: {prob.max().item():.3f} | Uncertainty: {uncertainty.item():.6} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meGAyXatXYeR"
      },
      "source": [
        "## UQ Techniques: Conformal Uncertainty\n",
        "\n",
        "The initial step in utilizing conformal uncertainty is to assign an unconformal score (also known as an uncertainty score) to each data point in our calibration set (this is also called the fitting stage). The simplest way to define a score function is by subtracting the probabilities of the model's predictions from 1. Our calibration data scores will be sorted and saved as reference scores for our conformal predictor. Please note that we use `netcal` to calibrate our model's predictions and then use the calibrated predictions to calculate the reference scores (both of these processes are referred to as \"calibration,\" but they refer to distinct processes)!\n",
        "\n",
        "The next step is to predict the uncertainty of our model's previously made test predictions. In order to obtain a score for each of the test predictions, we employ the same scoring function as before. Then, we compare each test score to the previously established reference scores to determine how many of those reference scores are higher than the test score. The ratio of this number to the total number of reference scores yields the p-value for this particular prediction. (This is not the same value as the traditional statistical p-value!) The greater the p-value, the greater confidence we have in our model's prediction. Please note that since we will be utilizing Mondrian Conformal Prediction, we will repeat this procedure independently for each of the class probabilities in our model's predictions.\n",
        "\n",
        "\n",
        "We then compare the p-values of the predictions to a threshold value (usually 0.05 or 0.1) to determine whether the model's prediction is sufficiently confident. If the p-value is greater than the threshold, we can conclude that the model is confident in its prediction and use it accordingly. If the p-value is less than the threshold, we can conclude that the model's prediction is not supported by sufficient evidence and we should not use it.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        ">\n",
        "The following class has two main methods: 1) \"fit\", which is used to calculate the reference scores, and 2) \"predict\", which is used to calculate the p-values for the test predictions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2tZWhCrXYeS"
      },
      "outputs": [],
      "source": [
        "class MondrianConformalPredictor():\n",
        "    def __init__(self, num_classes: int):\n",
        "        \"\"\"A conformal predictor based on Mondrian forests.\n",
        "        Reference: https://arxiv.org/pdf/2107.07511.pdf\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.reference_score_dict = None\n",
        "\n",
        "    def fit(self, calibration_probs: np.ndarray, calibration_labels: np.ndarray) -> dict:\n",
        "        \"\"\"Fit the predictor to calibration data and calculate reference scores.\n",
        "        Reference scores are calculated based on the calibration data and will be used to calculate p-values later on,\n",
        "            so that's why they are named as \"reference\" scores.\n",
        "        Reference scores for each class will be collected in a dictionary with class indices as keys.\n",
        "        Please note that each reference score is a 1D array, however, the length of reference score arrays will\n",
        "            be different for different classes (as we are dealing with a presumably imbalanced dataset). This is\n",
        "            the reason that we use a dictionary to store reference scores for different classes instead of a 2-D array.\n",
        "        Args:\n",
        "            calibration_labels (np.ndarray): Labels for one fold of calibration data\n",
        "            calibration_probs (np.ndarray): Probabilities for one fold of calibration data\n",
        "        Returns:\n",
        "            reference_score_dict (dict): A dictionary of sorted reference scores for each class. Sorting is important\n",
        "                for calculating p-values later on.\n",
        "        \"\"\"\n",
        "        reference_score_dict = dict()\n",
        "        calibration_scores = 1 - calibration_probs\n",
        "        for class_idx in range(self.num_classes):\n",
        "            scores = calibration_scores[:, class_idx]\n",
        "            selected_scores = np.where(calibration_labels == class_idx, scores, -1)\n",
        "            selected_scores = np.sort(selected_scores[selected_scores != -1], axis=0) # From smaller scores (more certain) to larger scores (less certain)\n",
        "            reference_score_dict[class_idx] = selected_scores\n",
        "        self.reference_score_dict = reference_score_dict\n",
        "\n",
        "    def _calculate_p_values(self, test_scores: np.ndarray, reference_score_dict: dict) -> np.ndarray:\n",
        "        \"\"\"Calculate the p-values for the test scores based on the reference scores for the calibration data.\n",
        "        Args:\n",
        "            test_scores (np.ndarray): Calculated non-conformal scores for test data\n",
        "            reference_score_dict (dict): A dictionary of sorted reference scores for one fold of calibration data\n",
        "        Returns:\n",
        "            test_pvalues (np.ndarray): P-values for test scores\n",
        "        \"\"\"\n",
        "        test_pvalues = np.zeros((test_scores.shape[0], self.num_classes))\n",
        "        for class_idx in range(self.num_classes):\n",
        "\n",
        "            # Finding the rank of the test scores w.r.t. the reference scores\n",
        "            reference_scores = reference_score_dict[class_idx]\n",
        "            num_reference_scores = reference_scores.shape[0]\n",
        "            test_ranks = num_reference_scores - np.searchsorted(reference_scores, test_scores[:, class_idx], side='left')\n",
        "\n",
        "            # Calculating the p-values\n",
        "            test_pvalues[:, class_idx] = (test_ranks+1) / (num_reference_scores+1)\n",
        "\n",
        "        return test_pvalues\n",
        "\n",
        "    def predict(self, test_probs: np.ndarray, error_rate: float=0.2) -> list:\n",
        "        \"\"\"Predict the confident (certain) classes for test data\n",
        "        Args:\n",
        "            test_probs (np.ndarray): Probabilities for test data\n",
        "            error_rate (float): the desirable error rate (tolerable error due to uncertainty)\n",
        "        Returns:\n",
        "            certain_predictions (list): List of confident (certain) classes for each test sample\n",
        "            certain_probabilities (list): List of confident (certain) probabilities for each test sample\n",
        "            certain_scores (list): List of confident (certain) scores for each test sample\n",
        "        \"\"\"\n",
        "        assert self.reference_score_dict is not None, \"Please fit the predictor first!\"\n",
        "\n",
        "        # Calculating the sample p-values for multiple reference folds and averaging them\n",
        "        test_scores = 1 - test_probs\n",
        "        test_pvalues = np.array(self._calculate_p_values(test_scores, self.reference_score_dict))\n",
        "\n",
        "        # Sample certainties are 1 if the sample p-value is greater than the error rate\n",
        "        sample_certainties = (test_pvalues > error_rate).astype(int)\n",
        "        certain_predictions = [np.where(sample_certainties[i, :]==1)[0] for i in range(sample_certainties.shape[0])]\n",
        "        certain_probablities = [test_probs[i, sample_certainties[i, :]==1] for i in range(sample_certainties.shape[0])]\n",
        "        certain_pvalues = [test_pvalues[i, sample_certainties[i, :]==1] for i in range(sample_certainties.shape[0])]\n",
        "\n",
        "        return certain_predictions, certain_probablities, certain_pvalues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggMtkOyHXYeS"
      },
      "source": [
        "Now let's see how our conformal predictor will alter the predictions of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unlg441RXYeS"
      },
      "outputs": [],
      "source": [
        "y_preds, y_true = get_inference_results(\"CE_seed_42.pth\", set_name='test')\n",
        "\n",
        "# Calibrate the predictions\n",
        "calibrator = IsotonicRegression()\n",
        "calibrator.fit(y_preds, y_true)\n",
        "y_preds_calibrated = calibrator.transform(y_preds)[:, None]\n",
        "y_preds_calibrated = np.concatenate([y_preds_calibrated, 1-y_preds_calibrated], axis=1)\n",
        "\n",
        "# Mondrian Conformal Predictor\n",
        "mccp = MondrianConformalPredictor(num_classes=2)\n",
        "mccp.fit(y_preds, y_true)\n",
        "certain_predictions, certain_probablities, certain_pvalues = mccp.predict(y_preds, error_rate=0.1)\n",
        "\n",
        "for i, (label, prediction, probablity, pvalue) in enumerate(zip(y_true, certain_predictions, certain_probablities, certain_pvalues)):\n",
        "    print(f\"Label: {label} | Prediction set: {prediction} | Probablity: {probablity} | P-value: {pvalue}\")\n",
        "    if i == 20:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UL97M-XYeS"
      },
      "source": [
        "As you can see above, there are sevral predictions that have more than one label in their prediction sets. This simply means that our current model thinks at an error rate of 0.1 (meaning we accept 10% incorrect predictions), both labels could be positive for those predictions. But you can also see a few others which have only one label in their prediction sets. This means that our model is confident in its prediction and we can use it accordingly. Our model confidence may improve if we train a better model, but even right now, you can play with the threshold value and see how it affects the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7uuwZyGXYeS"
      },
      "source": [
        "### Thank you to Dr. Bardia Khosravi, MD, MPH, and Dr. Pouria Rouzrokh, MD, MPH, for their invaluable assistance in preparing this notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}